{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install datasets transformers accelerate torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, BlipForConditionalGeneration, BlipProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_dict: dict):\n",
    "    \"\"\"Perform basic analysis on the dataset.\"\"\"\n",
    "    for split in dataset_dict.keys():\n",
    "        dataset = dataset_dict[split]\n",
    "        print(f\"Dataset Split: {split}\")\n",
    "        print(f\"Number of Samples: {len(dataset)}\")\n",
    "        print(f\"Features: {dataset.features.keys()}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = {col: sum(1 for x in dataset[col] if x is None) for col in dataset.features.keys()}\n",
    "        print(f\"Missing values: {missing_values}\")\n",
    "        \n",
    "        # Show a few sample images with captions\n",
    "        sample = dataset[0]\n",
    "        image = sample['image']\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = image.convert('RGB')\n",
    "        plt.imshow(image)\n",
    "        plt.title(sample['label'])\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"jmhessel/newyorker_caption_contest\", \"matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Handle variable-sized inputs here\n",
    "    images = [item['image'] for item in batch]\n",
    "    captions = [item['label'] for item in batch]\n",
    "    # Resize images to the same size\n",
    "    images = torch.stack([transforms.Resize((224, 224))(image) for image in images])\n",
    "    # Convert captions to tensor\n",
    "    captions = torch.stack(captions)\n",
    "    return {'image': images, 'label': captions}\n",
    "\n",
    "# evauate model\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model performance on validation or test dataset.\"\"\"\n",
    "    print(\"batch -----------> \", )\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            captions = batch['label'].to(device)\n",
    "            outputs = model(pixel_values=images, labels=captions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataset_dict):\n",
    "    \"\"\"Test the trained model on the test dataset.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "    test_dataloader = DataLoader(dataset_dict['test'], batch_size=8, shuffle=False)\n",
    "    test_loss = evaluate_model(model, test_dataloader, nn.CrossEntropyLoss(), device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following steps are perfomed:\n",
    "##### 1. Load model and processor\n",
    "##### 2. Preprocessing function\n",
    "##### 3. Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(example):\n",
    "    image = example[\"image\"].convert(\"RGB\")  # Convert grayscale to RGB\n",
    "    example[\"image\"] = transform(image)  # Apply transforms\n",
    "\n",
    "    # Convert label ('A', 'B', etc.) to integer index\n",
    "    label_idx = ord(example[\"label\"]) - ord(\"A\")\n",
    "    caption = example[\"caption_choices\"][label_idx]\n",
    "\n",
    "    # Tokenize with padding and ensure output is a tensor\n",
    "    tokenized = processor.tokenizer(\n",
    "        caption, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "    tokenized = tokenized.squeeze(0) if tokenized.dim() > 1 else tokenized\n",
    "    example[\"label\"] = tokenized  # This should be a torch.Tensor\n",
    "    return example\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = load_dataset(\"jmhessel/newyorker_caption_contest\", \"matching\")\n",
    "processed_data = data.map(preprocess_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for batching\n",
    "def collate_fn(batch):\n",
    "    print(\"batch inside collate function-----------> \", len(batch))\n",
    "    valid_items = []\n",
    "    for item in batch:\n",
    "        try:\n",
    "            # Ensure image and label are tensors (if not, try converting them)\n",
    "            if not isinstance(item[\"image\"], torch.Tensor):\n",
    "                item[\"image\"] = torch.tensor(item[\"image\"])\n",
    "            if not isinstance(item[\"label\"], torch.Tensor):\n",
    "                item[\"label\"] = torch.tensor(item[\"label\"], dtype=torch.long)\n",
    "            valid_items.append(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item: {e}\")\n",
    "            # Skip this item if conversion fails\n",
    "            pass\n",
    "\n",
    "    if not valid_items:\n",
    "        # If no valid items, return an empty dict or handle appropriately\n",
    "        return {\"image\": None, \"label\": None}\n",
    "\n",
    "    images = torch.stack([i[\"image\"] for i in valid_items])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [i[\"label\"] for i in valid_items], batch_first=True, padding_value=0\n",
    "    )\n",
    "    return {\"image\": images, \"label\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    processed_data[\"train\"], batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "print(\"Dataloader created!\")\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "print(\"Starting training loop...\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        print(\"batch -----------> \", len(batch))\n",
    "        if batch[\"image\"] is None:\n",
    "            print(\"Skipping batch due to missing data\")\n",
    "            continue  # Skip batch if no valid items\n",
    "        images = batch[\"image\"].to(device)\n",
    "        captions = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=images, input_ids=captions, labels=captions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss:.4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"blip_captioning_model.pth\")\n",
    "print(\"Model saved to blip_captioning_model.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model architecture\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.load_state_dict(torch.load(\"blip_captioning_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "print(\"Model loaded successfully for evaluation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image from the test dataset\n",
    "sample_image = data[\"test\"][0][\"image\"]  # Assuming data is still loaded\n",
    "sample_image.show()  # Display the image (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data):\n",
    "    results = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    counter = 0\n",
    "    # print(test_data[0])\n",
    "    for i in range(len(test_data)):\n",
    "        image_path = test_data[i][\"image\"]\n",
    "        print(image_path)\n",
    "        counter += 1\n",
    "        print(test_data[0])\n",
    "        ground_truth = test_data[i][\"label\"]\n",
    "        try:\n",
    "            generated_caption = generate_caption(image_path)\n",
    "            # Store results\n",
    "            predictions.append(generated_caption)\n",
    "            references.append(ground_truth)\n",
    "            results.append([image_path, ground_truth, generated_caption])\n",
    "        except Exception as error:\n",
    "            generated_caption = generate_caption(image_path)\n",
    "            # Store results\n",
    "            predictions.append(\"error\")\n",
    "            references.append(\"error\")\n",
    "            results.append([image_path, ground_truth, generated_caption])\n",
    "\n",
    "    # Compute automatic scores\n",
    "    scores = compute_metrics(predictions, references)\n",
    "    print(\"Evaluation Metrics:\", scores)\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results, columns=[\"Image Path\", \"Ground Truth\", \"Generated Caption\"])\n",
    "    df.to_csv(\"caption_results.csv\", index=False)\n",
    "    print(\"Evaluation completed. Results saved in caption_results.csv\")\n",
    "\n",
    "test_data = processed_dataset[\"test\"]\n",
    "evaluate_model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {\n",
    "    \"A\" : 0,\n",
    "    \"B\" : 1,\n",
    "    \"C\" : 2,\n",
    "    \"D\" : 3,        \n",
    "    \"E\" : 4,\n",
    "    \"F\" : 5,\n",
    "    \"G\" : 6,\n",
    "    \"H\" : 7,\n",
    "    \"I\" : 8,\n",
    "    \"J\" : 9,\n",
    "    \"K\" : 10,\n",
    "    \"L\" : 11,\n",
    "    \"M\" : 12,\n",
    "    \"N\" : 13,\n",
    "    \"O\" : 14,\n",
    "    \"P\" : 15,\n",
    "    \"Q\" : 16,\n",
    "    \"R\" : 17,\n",
    "    \"S\" : 18,\n",
    "    \"T\" : 19,\n",
    "    \"U\" : 20,\n",
    "    \"V\" : 21,\n",
    "    \"W\" : 22,\n",
    "    \"X\" : 23,\n",
    "    \"Y\" : 24,\n",
    "    \"Z\" : 25    \n",
    "}\n",
    "# Process the image for the model\n",
    "inputs = processor(images=sample_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate a caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_length=50)\n",
    "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print results\n",
    "print(\"Generated Caption:\", generated_caption)\n",
    "print(\"Ground Truth Caption:\", data[\"test\"][0][\"label\"])  # Assuming label stores the actual caption\n",
    "print(\"Image Location:\", data[\"test\"][0][\"image_location\"])  # Assuming image location is stored\n",
    "print(\"Image Description:\", data[\"test\"][0][\"image_description\"])  # Assuming image\n",
    "print(\"Image Uncanny Description:\", data[\"test\"][0][\"image_uncanny_description\"])  # Assuming image uncanny description\n",
    "print(\"Caption Choices selected:\", data[\"test\"][0][\"caption_choices\"][label[data[\"test\"][0][\"label\"]]])  # Assuming caption choices are stored\n",
    "print(\"Caption Choices:\", data[\"test\"][0][\"caption_choices\"])  # Assuming caption choices are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the model into transformer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "# notebook_login()\n",
    "\n",
    "# Define model directory and Hugging Face repo\n",
    "model_dir = \"blip_caption_model\"\n",
    "hf_repo = \"Nishthaaa/image_captioning\"\n",
    "\n",
    "# Load processor (update to match your training processor)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load your `.pth` model\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")  # Base model\n",
    "model.load_state_dict(torch.load(\"blip_captioning_model.pth\", map_location=\"cpu\"))  # Load trained weights\n",
    "\n",
    "# Save processor and model\n",
    "processor.save_pretrained(model_dir)\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# # Push to Hugging Face Model Hub\n",
    "# processor.push_to_hub(hf_repo)\n",
    "# model.push_to_hub(hf_repo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On your terminal do the following steps:\n",
    "# 1. Login to huggingface via your token\n",
    "# 2. create the space on hugging face\n",
    "# 3. push the model to hugging face via the below code\n",
    "# 4. check via streamlit app which is created\n",
    "from huggingface_hub import Repository, HfApi\n",
    "import torch\n",
    "\n",
    "# Set your Hugging Face repo name\n",
    "hf_repo_name = \"Nishthaaa/image_captioning\"\n",
    "# Clone the repo locally\n",
    "repo = Repository(local_dir=\"blip_caption_model\", clone_from=f\"https://huggingface.co/{hf_repo_name}\")\n",
    "# Save the model\n",
    "model_path = \"blip_caption_model\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Push to Hugging Face\n",
    "repo.push_to_hub(commit_message=\"Upload trained BLIP captioning model\")\n",
    "print(\"Model pushed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
